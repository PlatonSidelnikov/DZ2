{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlatonSidelnikov/DZ2/blob/main/%D0%94%D0%97_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvtMXse1gvHl"
      },
      "source": [
        "# Домашнее задание 6: классификация текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITiR2_2gvHo"
      },
      "source": [
        "В этом домашнем задании вам предстоит построить классификатор текстов!\n",
        "\n",
        "Данные мы будем использовать из Kaggle соревнования: https://www.kaggle.com/competitions/nlp-getting-started/data Оттуда надо скачать файл train.csv. На обучающую и тестовую выборки его поделим кодом ниже, менять его не надо!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFjvNFEpgvHp"
      },
      "source": [
        "Мы будем работать с датасетом постов из твиттера. Нам предстоит решать задачу бинарной классификации - определять содержатся ли в твитте информация о настоящей катастрофе/инциденте или нет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD2a_g8wgvHp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/PlatonSidelnikov/Dz-6/main/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B22hb4QhjMqx",
        "outputId": "1604ac84-e140-4b23-f9c8-70b03cfba7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-09 07:32:25--  https://raw.githubusercontent.com/PlatonSidelnikov/Dz-6/main/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 987712 (965K) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>] 964.56K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-05-09 07:32:25 (19.9 MB/s) - ‘train.csv’ saved [987712/987712]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TL97PbQgvHq"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi5pQgkfgvHq",
        "outputId": "823b5130-e53a-47fe-9b2a-d3b922ed9736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-10cb78e4-aa90-4b98-bbdc-f79f25eb5d84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10cb78e4-aa90-4b98-bbdc-f79f25eb5d84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-10cb78e4-aa90-4b98-bbdc-f79f25eb5d84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-10cb78e4-aa90-4b98-bbdc-f79f25eb5d84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISmXHiy5gvHr"
      },
      "outputs": [],
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train, test = train_test_split(data, test_size=0.3, random_state=42)\n",
        "\n",
        "# применим разбиение на тренировочную и тестовую выборки уже после всех преобразований дата фрейма"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6ngXOEGgvHs"
      },
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "\n",
        "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of NaNs:\\n')\n",
        "print(data.isnull().sum(axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne-3szzAkTR-",
        "outputId": "e2e4d43c-6dbb-4162-87ef-d0a60cb3aeda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaNs:\n",
            "\n",
            "id             0\n",
            "keyword       61\n",
            "location    2533\n",
            "text           0\n",
            "target         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_nan_data = data.fillna('')\n",
        "print('Number of NaNs:\\n')\n",
        "print(no_nan_data.isnull().sum(axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh4yHZZ1lK1X",
        "outputId": "18c14ebe-90dc-4845-ed6b-fe0b8ce01374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaNs:\n",
            "\n",
            "id          0\n",
            "keyword     0\n",
            "location    0\n",
            "text        0\n",
            "target      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# применим разбиение на новом датасете без нанов\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(no_nan_data, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "Mi1AeBFX9eKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqeIDioEgvHt"
      },
      "source": [
        "## Задание 2 (1 балл)\n",
        "Давайте немного посмотрим на наши данные. Визуализируйте (где явно просят) или выведете информацию о следующем:\n",
        "\n",
        "1. Какое распределение классов в обучающей выборке?\n",
        "2. Посмотрите на колонку \"keyword\" - возьмите 10 наиболее встречающихся значений, постройте ступенчатую диаграмму распределения классов в зависимости от значения keyword, сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Посмотрим на соотношение классов в выборке, выведем долю каждого класса\n",
        "a = train['target'].sum() / len(train['target'])\n",
        "print(f'share of 1: {a}')\n",
        "print(f'share of 0: {1 - a}')\n",
        "#распределение классов в обучающей выборке близко к 50 на 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fymUukJm-vj",
        "outputId": "238c647b-cfb2-44d2-8df1-4bafbb79a118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "share of 1: 0.43253893788703324\n",
            "share of 0: 0.5674610621129668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = dict(data['keyword'].value_counts().head(10))\n",
        "keywords = list(b.keys())\n",
        "mean_values = []\n",
        "for i in keywords:\n",
        "  a = no_nan_data[data['keyword'] == i]['target'].mean()\n",
        "  mean_values.append(a)\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "ax = fig.add_subplot()\n",
        "ax.step(keywords, mean_values)\n",
        "plt.show()\n",
        "\n",
        "# построив ступенчатую диаграмму, можно увидеть, что среди наиболее часто встречаемых ключевых слов только body%20bags, outbreak, evacuate,\n",
        "# fatalities, sinking и damage указывают, что переменная target скорее всего примет значение 1\n",
        "# кроме того, по графику можем сделать вывод, что частота ключевого слова особо не влияет на целевую переменную"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "BtBDMw0LqPzT",
        "outputId": "07306ff4-5ac7-4dda-fea7-f4620c761d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlnWB8TlgvHt"
      },
      "source": [
        "## Задание 3 (0.5 балла) \n",
        "\n",
        "В этом задании предлагается объединить все три текстовых столбца в один (просто сконкатенировать cтроки) и убрать столбец с индексом."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_nan_data['str'] = no_nan_data[['keyword', 'location', 'text']].apply(lambda x: ' '.join(x), axis=1)"
      ],
      "metadata": {
        "id": "mUIkBjOaFvJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del no_nan_data['keyword'], no_nan_data['id'], no_nan_data['location'], no_nan_data['text']"
      ],
      "metadata": {
        "id": "xRnSVX08EM4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_nan_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6edzSlWYEemB",
        "outputId": "5e4babd4-dbe7-44f9-b649-30db6f90c2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      target                                                str\n",
              "0          1    Our Deeds are the Reason of this #earthquake...\n",
              "1          1             Forest fire near La Ronge Sask. Canada\n",
              "2          1    All residents asked to 'shelter in place' ar...\n",
              "3          1    13,000 people receive #wildfires evacuation ...\n",
              "4          1    Just got sent this photo from Ruby #Alaska a...\n",
              "...      ...                                                ...\n",
              "7608       1    Two giant cranes holding a bridge collapse i...\n",
              "7609       1    @aria_ahrary @TheTawniest The out of control...\n",
              "7610       1    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. h...\n",
              "7611       1    Police investigating after an e-bike collide...\n",
              "7612       1    The Latest: More Homes Razed by Northern Cal...\n",
              "\n",
              "[7613 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-70c54d85-38f2-4da1-869c-15f0eab9c979\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to 'shelter in place' ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7608</th>\n",
              "      <td>1</td>\n",
              "      <td>Two giant cranes holding a bridge collapse i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7609</th>\n",
              "      <td>1</td>\n",
              "      <td>@aria_ahrary @TheTawniest The out of control...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7610</th>\n",
              "      <td>1</td>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7611</th>\n",
              "      <td>1</td>\n",
              "      <td>Police investigating after an e-bike collide...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>1</td>\n",
              "      <td>The Latest: More Homes Razed by Northern Cal...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70c54d85-38f2-4da1-869c-15f0eab9c979')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70c54d85-38f2-4da1-869c-15f0eab9c979 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70c54d85-38f2-4da1-869c-15f0eab9c979');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# разобьем на тренировочную и тестовые выборки новый датасет из 2-х колонок, очищенный от индексов и с объединенными строками\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train1, test1 = train_test_split(no_nan_data, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "kLhvYU8GEu4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvNszQzgvHu"
      },
      "source": [
        "## Задание 4 (0.5 балла)\n",
        "\n",
        "Далее мы будем пока работать только с train частью.\n",
        "\n",
        "1. Предобработайте данные (train часть) с помощью CountVectorizer.\n",
        "2. Какого размера получилась матрица?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#сформируем тренировочные и тестовые выборки\n",
        "# поскольку токенайзер принимает только список строк, преобразуем выборки из списка списков в список строк\n",
        "X_train = list(map(lambda x: ''.join(x), np.array(train1.drop('target', axis=1))))\n",
        "y_train = train1['target']\n",
        "X_test = list(map(lambda x: ''.join(x), np.array(test1.drop('target', axis=1))))\n",
        "y_test = test1['target']"
      ],
      "metadata": {
        "id": "t6SOQhSVQgDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "vectorizer.vocabulary_\n",
        "vector = vectorizer.transform(X_train)\n",
        "vector.shape\n",
        "# матрица получилась довольно большого размера"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd4PFenRZ__C",
        "outputId": "5cf84d6d-63e1-4c53-d004-0d4e2fa4c517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5329, 18455)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9J9LBaUgvHu"
      },
      "source": [
        "## Задание 5 (1 балл)\n",
        "\n",
        "В предыдущем пункте у вас должна была получиться достаточно большая матрица.\n",
        "Если вы взгляните на текст, то увидете, что там есть множество специальных символов, ссылок и прочего мусора.\n",
        "\n",
        "Давайте также посмотрим на словарь, который получился в результате построения CountVectorizer, его можно найти в поле vocabulary_ инстанса этого класса. Давайте напишем функцию, которая печает ответы на следующие вопросы:\n",
        "\n",
        "1. Найдите в этом словаре все слова, которые содержат цифры. Сколько таких слов нашлось?\n",
        "\n",
        "2. Найдите все слова, которые содержат символы пунктуации. Сколько таких слов нашлось? \n",
        "\n",
        "3. Сколько хэштегов (токен начинается на #) и упоминаний (токен начинается на @) осталось в словаре?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на значения в полученном словаре\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHzCE5MDkzEX",
        "outputId": "40d671e1-c056-4519-b81d-a7ea6cf8d51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bridge': 2948,\n",
              " '20collapse': 320,\n",
              " 'ashes': 1928,\n",
              " '2015': 295,\n",
              " 'australia': 2059,\n",
              " 'ûªs': 18425,\n",
              " 'collapse': 3914,\n",
              " 'at': 1977,\n",
              " 'trent': 16483,\n",
              " 'among': 1628,\n",
              " 'worst': 17813,\n",
              " 'in': 8314,\n",
              " 'history': 7773,\n",
              " 'england': 5722,\n",
              " 'bundled': 3085,\n",
              " 'out': 11995,\n",
              " 'for': 6503,\n",
              " '60': 755,\n",
              " 'http': 7970,\n",
              " 'co': 3861,\n",
              " 't5trhjuau0': 15733,\n",
              " 'hail': 7420,\n",
              " 'carol': 3363,\n",
              " 'stream': 15419,\n",
              " 'illinois': 8249,\n",
              " 'great': 7206,\n",
              " 'michigan': 10594,\n",
              " 'technique': 15888,\n",
              " 'camp': 3261,\n",
              " 'b1g': 2147,\n",
              " 'thanks': 16013,\n",
              " 'to': 16272,\n",
              " 'bmurph1019': 2753,\n",
              " 'hail_youtsey': 7421,\n",
              " 'termn8r13': 15949,\n",
              " 'goblue': 7088,\n",
              " 'wrestleon': 17848,\n",
              " 'oaskgki6qj': 11648,\n",
              " 'police': 12634,\n",
              " 'houston': 7933,\n",
              " 'cnn': 3854,\n",
              " 'tennessee': 15932,\n",
              " 'movie': 10940,\n",
              " 'theater': 16025,\n",
              " 'shooting': 14647,\n",
              " 'suspect': 15632,\n",
              " 'killed': 9246,\n",
              " 'by': 3150,\n",
              " 'di8elzswnr': 4890,\n",
              " 'rioting': 13800,\n",
              " 'still': 15357,\n",
              " 'couple': 4204,\n",
              " 'of': 11708,\n",
              " 'hours': 7925,\n",
              " 'left': 9644,\n",
              " 'until': 16884,\n",
              " 'have': 7546,\n",
              " 'be': 2383,\n",
              " 'up': 16895,\n",
              " 'class': 3774,\n",
              " 'wounds': 17822,\n",
              " 'lake': 9485,\n",
              " 'highlands': 7724,\n",
              " 'crack': 4242,\n",
              " 'the': 16022,\n",
              " 'path': 12228,\n",
              " 'where': 17582,\n",
              " 'wiped': 17684,\n",
              " 'this': 16107,\n",
              " 'morning': 10886,\n",
              " 'during': 5348,\n",
              " 'beach': 2386,\n",
              " 'run': 14049,\n",
              " 'surface': 15606,\n",
              " 'on': 11828,\n",
              " 'elbow': 5571,\n",
              " 'and': 1651,\n",
              " 'right': 13783,\n",
              " 'knee': 9316,\n",
              " 'yaqrsximph': 18081,\n",
              " 'airplane': 1458,\n",
              " '20accident': 309,\n",
              " 'somewhere': 15025,\n",
              " 'there': 16070,\n",
              " 'experts': 5988,\n",
              " 'france': 6594,\n",
              " 'begin': 2442,\n",
              " 'examining': 5934,\n",
              " 'debris': 4641,\n",
              " 'found': 6553,\n",
              " 'reunion': 13707,\n",
              " 'island': 8601,\n",
              " 'french': 6642,\n",
              " 'air': 1451,\n",
              " 'accident': 1241,\n",
              " 'tagzbcxfj0': 15758,\n",
              " 'mlb': 10762,\n",
              " 'bloody': 2719,\n",
              " 'isolated': 8610,\n",
              " 'city': 3742,\n",
              " 'world': 17794,\n",
              " 'perth': 12370,\n",
              " 'came': 3251,\n",
              " 'kill': 9243,\n",
              " 'indians': 8356,\n",
              " 'fun': 6721,\n",
              " 'video': 17164,\n",
              " 'smirking': 14922,\n",
              " 'remorseless': 13579,\n",
              " 'pakistani': 12124,\n",
              " 'killer': 9247,\n",
              " 'shows': 14678,\n",
              " 'him': 7746,\n",
              " 'boasting': 2766,\n",
              " 'fpjlwoxklg': 6575,\n",
              " 'burning': 3101,\n",
              " 'johnsontionne': 8903,\n",
              " 'except': 5939,\n",
              " 'idk': 8176,\n",
              " 'them': 16055,\n",
              " 'it': 8620,\n",
              " 'really': 13423,\n",
              " 'destroy': 4823,\n",
              " 'he': 7580,\n",
              " 'or': 11919,\n",
              " 'she': 14590,\n",
              " 'her': 7670,\n",
              " 'ask': 1943,\n",
              " 'house': 7926,\n",
              " 'wounded': 17821,\n",
              " 'maracay': 10278,\n",
              " 'nirgua': 11374,\n",
              " 'venezuela': 17104,\n",
              " 'officer': 11725,\n",
              " 'dead': 4615,\n",
              " 'after': 1395,\n",
              " 'exchanging': 5943,\n",
              " 'shots': 14660,\n",
              " 'xxfk4khbiw': 18027,\n",
              " 'wreck': 17845,\n",
              " 'currently': 4410,\n",
              " 'writing': 17858,\n",
              " 'book': 2805,\n",
              " 'friggin': 6666,\n",
              " 'destiel': 4817,\n",
              " 'sucks': 15514,\n",
              " 'read': 13402,\n",
              " 'vine': 17185,\n",
              " 'description': 4797,\n",
              " 'https': 7971,\n",
              " 'mkx6ux4ozt': 10760,\n",
              " 'mudslide': 10998,\n",
              " 'malibu': 10234,\n",
              " 'santafe': 14210,\n",
              " 'winning': 17679,\n",
              " 'sterling': 15340,\n",
              " 'scott': 14326,\n",
              " 'red': 13475,\n",
              " 'carpet': 3368,\n",
              " 'fundraiser': 6724,\n",
              " 'oso': 11969,\n",
              " 'ma4ra7atql': 10151,\n",
              " 'cg579wldne': 3513,\n",
              " 'casualties': 3406,\n",
              " 'canadian': 3282,\n",
              " 'bread': 2915,\n",
              " 'libertarianluke': 9733,\n",
              " 'all': 1539,\n",
              " 'that': 16016,\n",
              " 'honest': 7859,\n",
              " 'if': 8187,\n",
              " 'people': 12330,\n",
              " 'want': 17376,\n",
              " 'go': 7080,\n",
              " 'rampage': 13313,\n",
              " 'let': 9698,\n",
              " 'use': 16955,\n",
              " 'their': 16050,\n",
              " 'own': 12051,\n",
              " 'hands': 7465,\n",
              " 'feet': 6221,\n",
              " 'no': 11417,\n",
              " 'ambulance': 1608,\n",
              " 'amsterdam': 1635,\n",
              " '7xglah10zl': 928,\n",
              " 'twelve': 16646,\n",
              " 'feared': 6198,\n",
              " 'helicopter': 7646,\n",
              " 'crash': 4257,\n",
              " 'thmblaatzp': 16115,\n",
              " 'electrocuted': 5585,\n",
              " 'got': 7141,\n",
              " 'last': 9543,\n",
              " 'night': 11359,\n",
              " 'work': 17784,\n",
              " 'first': 6348,\n",
              " 'time': 16206,\n",
              " 'my': 11069,\n",
              " 'life': 9746,\n",
              " 'shit': 14630,\n",
              " 'was': 17406,\n",
              " 'weird': 17514,\n",
              " 'drown': 5276,\n",
              " 'some': 15013,\n",
              " 'older': 11801,\n",
              " 'native': 11173,\n",
              " 'australians': 2061,\n",
              " 'believe': 2469,\n",
              " 'oceans': 11685,\n",
              " 'were': 17532,\n",
              " 'created': 4278,\n",
              " 'from': 6673,\n",
              " 'urine': 16932,\n",
              " 'an': 1638,\n",
              " 'angry': 1676,\n",
              " 'god': 7090,\n",
              " 'who': 17605,\n",
              " 'tried': 16503,\n",
              " 'volcano': 17257,\n",
              " 'west': 17537,\n",
              " 'coast': 3869,\n",
              " 'cali': 3226,\n",
              " 'usa': 16943,\n",
              " 'architect': 1834,\n",
              " 'behind': 2454,\n",
              " 'kanye': 9086,\n",
              " 'musbik7ejf': 11030,\n",
              " 'attack': 2002,\n",
              " 'mumbai': 11017,\n",
              " 'india': 8349,\n",
              " 'shud': 14684,\n",
              " 'not': 11481,\n",
              " 'give': 7013,\n",
              " 'any': 1734,\n",
              " 'evidence': 5919,\n",
              " 'pak': 12122,\n",
              " 'they': 16089,\n",
              " 'will': 17641,\n",
              " 'share': 14573,\n",
              " 'with': 17700,\n",
              " 'terrorists': 15962,\n",
              " 'amp': 1630,\n",
              " 'next': 11313,\n",
              " 'oth': 11977,\n",
              " 'contries': 4114,\n",
              " 'qiopbtiuvu': 13126,\n",
              " 'body': 2774,\n",
              " '20bag': 311,\n",
              " 'new': 11281,\n",
              " 'york': 18173,\n",
              " 'auth': 2063,\n",
              " 'louis': 9993,\n",
              " 'vuitton': 17294,\n",
              " 'brown': 3002,\n",
              " 'saumur': 14245,\n",
              " '35': 485,\n",
              " 'cross': 4322,\n",
              " 'shoulder': 14662,\n",
              " 'bag': 2202,\n",
              " 'monogram': 10842,\n",
              " '23': 355,\n",
              " '419': 581,\n",
              " 'full': 6718,\n",
              " 'û_': 18408,\n",
              " 'hcdiwe5flc': 7571,\n",
              " 'zlvebeoavg': 18338,\n",
              " 'annihilated': 1696,\n",
              " 'higher': 7721,\n",
              " 'places': 12532,\n",
              " 'episode': 5783,\n",
              " 'trunks': 16557,\n",
              " 'freiza': 6640,\n",
              " 'is': 8585,\n",
              " 'cleanest': 3782,\n",
              " 'ever': 5902,\n",
              " 'showed': 14674,\n",
              " 'nigga': 11357,\n",
              " 'mercy': 10518,\n",
              " 'cyclone': 4447,\n",
              " 'hyderabad': 8073,\n",
              " 'roughdeal1': 13969,\n",
              " 'ante': 1714,\n",
              " 'hudhud': 7978,\n",
              " 'chandrababu': 3544,\n",
              " 'valle': 17039,\n",
              " 'ne': 11224,\n",
              " 'ga': 6799,\n",
              " '65': 781,\n",
              " 'zhenghxn': 18312,\n",
              " '11': 117,\n",
              " 'eyes': 6036,\n",
              " 'akame': 1474,\n",
              " 'tokyo': 16296,\n",
              " 'ghoul': 6978,\n",
              " 'damn': 4511,\n",
              " 'dont': 5154,\n",
              " 'dare': 4551,\n",
              " 'watch': 17423,\n",
              " 'suicide': 15530,\n",
              " '20bombing': 317,\n",
              " 'principality': 12842,\n",
              " 'zeron': 18304,\n",
              " 'rayquazaerk': 13368,\n",
              " 'are': 1836,\n",
              " 'christian': 3689,\n",
              " 'sure': 15600,\n",
              " 'but': 3126,\n",
              " 'don': 5144,\n",
              " 'bombing': 2794,\n",
              " 'employed': 5668,\n",
              " 'often': 11746,\n",
              " 'as': 1912,\n",
              " 'islamic': 8598,\n",
              " 'groups': 7259,\n",
              " 'demolished': 4739,\n",
              " 'jackmulholland1': 8698,\n",
              " 'think': 16099,\n",
              " 'also': 1581,\n",
              " 'became': 2412,\n",
              " 'marquis': 10322,\n",
              " 'then': 16062,\n",
              " 'carlos': 3355,\n",
              " 'charlie': 3569,\n",
              " 'finally': 6308,\n",
              " 'dublin': 5308,\n",
              " 'sadly': 14130,\n",
              " 'inundated': 8515,\n",
              " 'surf': 15604,\n",
              " 'hi': 7711,\n",
              " 'waimea': 17339,\n",
              " 'bay': 2344,\n",
              " 'like': 9769,\n",
              " 'surfers': 15607,\n",
              " 'czdw8oowa2': 4460,\n",
              " 'collision': 3928,\n",
              " 'denver': 4763,\n",
              " 'colorado': 3939,\n",
              " 'motorcyclist': 10919,\n",
              " 'bicyclist': 2564,\n",
              " 'injured': 8415,\n",
              " 'broadway': 2984,\n",
              " 'zl7ojdaj3u': 18334,\n",
              " 'flames': 6386,\n",
              " 'around': 1879,\n",
              " 'you': 18177,\n",
              " 'maryland': 10334,\n",
              " 'mansion': 10268,\n",
              " 'fire': 6330,\n",
              " 'caused': 3434,\n",
              " 'damaged': 4502,\n",
              " 'plug': 12586,\n",
              " 'under': 16813,\n",
              " 'christmas': 3696,\n",
              " 'tree': 16469,\n",
              " 'report': 13616,\n",
              " 'says': 14264,\n",
              " 'into': 8509,\n",
              " 'lkjfabqzb3': 9869,\n",
              " 'demolish': 4738,\n",
              " 'nyhc': 11603,\n",
              " 'going': 7098,\n",
              " 'drake': 5213,\n",
              " 'over': 12023,\n",
              " 'ghostwriting': 6977,\n",
              " 'should': 14661,\n",
              " 'know': 9331,\n",
              " 'rihanna': 13788,\n",
              " 'lives': 9847,\n",
              " 'door': 5158,\n",
              " 'buildings': 3066,\n",
              " '20burning': 319,\n",
              " 'blue': 2736,\n",
              " 'yes': 18118,\n",
              " '1acd4900c1424d1': 232,\n",
              " 'foxnews': 6567,\n",
              " 'one': 11834,\n",
              " 'down': 5182,\n",
              " 'looting': 9961,\n",
              " 'forest': 6518,\n",
              " '20fires': 327,\n",
              " 'nicola': 11345,\n",
              " 'valley': 17040,\n",
              " 'fires': 6343,\n",
              " 'dying': 5384,\n",
              " 'salmon': 14165,\n",
              " 'act': 1279,\n",
              " 'deny': 4764,\n",
              " 'climate': 3805,\n",
              " 'change': 3545,\n",
              " 'nightmares': 11362,\n",
              " 'here': 7671,\n",
              " 'rbzomwgjee': 13380,\n",
              " 'bcpoli': 2375,\n",
              " 'canpoli': 3300,\n",
              " 'vanpoli': 17056,\n",
              " 'ns1aggfnxz': 11533,\n",
              " 'shoes': 14642,\n",
              " 'asics': 1940,\n",
              " 'gt': 7281,\n",
              " 'ii': 8217,\n",
              " 'super': 15573,\n",
              " 'ronnie': 13935,\n",
              " 'fieg': 6275,\n",
              " 'kith': 9287,\n",
              " 'white': 17599,\n",
              " '3m': 536,\n",
              " 'gel': 6908,\n",
              " 'grey': 7233,\n",
              " 'od250zshfy': 11689,\n",
              " 'sandstorm': 14201,\n",
              " 'airport': 1460,\n",
              " 'get': 6950,\n",
              " 'swallowed': 15645,\n",
              " 'minute': 10695,\n",
              " 'wd9odwjj9l': 17467,\n",
              " '20on': 331,\n",
              " '20fire': 326,\n",
              " 'uk': 16776,\n",
              " 'tweetlikeitsseptember11th2001': 16643,\n",
              " 'those': 16126,\n",
              " 'two': 16658,\n",
              " 'oil': 11769,\n",
              " '20spill': 338,\n",
              " 'ny': 11592,\n",
              " 'california': 3229,\n",
              " 'spill': 15149,\n",
              " 'might': 10622,\n",
              " 'larger': 9535,\n",
              " 'than': 16005,\n",
              " 'projected': 12889,\n",
              " 'xwxbyhtuzc': 18026,\n",
              " 'wzedxefblg': 17918,\n",
              " 'cartoon': 3380,\n",
              " 'bears': 2394,\n",
              " 'without': 17705,\n",
              " 'we': 17472,\n",
              " 'would': 17817,\n",
              " 'qave': 13098,\n",
              " 'knowlddge': 9333,\n",
              " 'toilet': 16293,\n",
              " 'paper': 12162,\n",
              " 'drought': 5271,\n",
              " 'miami': 10585,\n",
              " '_gaabyx': 1137,\n",
              " 'purple': 13035,\n",
              " 'activist': 1291,\n",
              " 'thought': 16129,\n",
              " 'injuries': 8417,\n",
              " 'madison': 10177,\n",
              " 'wi': 17620,\n",
              " 'st': 15242,\n",
              " 'mo': 10781,\n",
              " 'buffoonmike': 3060,\n",
              " 'knew': 9320,\n",
              " 'doing': 5128,\n",
              " 'much': 10995,\n",
              " 'bite': 2628,\n",
              " 'us': 16939,\n",
              " 'influenced': 8398,\n",
              " 'shitty': 14634,\n",
              " 'staff': 15251,\n",
              " 'acquisitions': 1274,\n",
              " 'landslide': 9511,\n",
              " 'austin': 2058,\n",
              " 'texas': 15977,\n",
              " 'toddstarnes': 16285,\n",
              " 'enjoy': 5729,\n",
              " 'impending': 8289,\n",
              " 'todd': 16283,\n",
              " 'hehe': 7632,\n",
              " 'apocalypse': 1768,\n",
              " 'oregon': 11935,\n",
              " 'look': 9946,\n",
              " 'grizzly': 7245,\n",
              " 'peak': 12297,\n",
              " 'now': 11512,\n",
              " 'looks': 9950,\n",
              " 'beginning': 2444,\n",
              " 'dystopian': 5389,\n",
              " 'detonation': 4841,\n",
              " 'ignition': 8203,\n",
              " 'knock': 9326,\n",
              " 'sensor': 14469,\n",
              " 'senso': 14468,\n",
              " 'standard': 15264,\n",
              " 'ks100': 9383,\n",
              " '7o4lnfbe7k': 920,\n",
              " 'fvzsgjtbew': 6753,\n",
              " '20responders': 334,\n",
              " 'week': 17505,\n",
              " 'responders': 13667,\n",
              " 'dart': 4564,\n",
              " 'members': 10495,\n",
              " 'participating': 12201,\n",
              " 'four': 6558,\n",
              " 'day': 4593,\n",
              " 'intensive': 8481,\n",
              " 'technical': 15886,\n",
              " 'large': 9534,\n",
              " 'animal': 1678,\n",
              " 'tl93aod3er': 16247,\n",
              " 'military': 10641,\n",
              " 'lot': 9984,\n",
              " '20': 281,\n",
              " 'tom': 16302,\n",
              " 'clancy': 3770,\n",
              " 'mystery': 11085,\n",
              " 'novels': 11510,\n",
              " 'paperback': 12163,\n",
              " 'obix79ncxn': 11654,\n",
              " 'tomclancy': 16306,\n",
              " 'drowning': 5278,\n",
              " 'coventry': 4216,\n",
              " 'why': 17618,\n",
              " 'low': 10009,\n",
              " 'self': 14439,\n",
              " 'image': 8264,\n",
              " 'take': 15766,\n",
              " 'quiz': 13198,\n",
              " 'z8r6r3nbtb': 18264,\n",
              " 'namffldh5h': 11138,\n",
              " 'gonna': 7110,\n",
              " 'fight': 6284,\n",
              " 'taylor': 15834,\n",
              " 'soon': 15044,\n",
              " 'danger': 4525,\n",
              " 'hailing': 7424,\n",
              " 'dayton': 4595,\n",
              " 'wish': 17695,\n",
              " 'could': 4190,\n",
              " 'victoria': 17154,\n",
              " 'secret': 14399,\n",
              " 'front': 6674,\n",
              " 'good': 7111,\n",
              " 'flood': 6428,\n",
              " 'spot': 15181,\n",
              " 'combo': 3950,\n",
              " '53inch': 689,\n",
              " '300w': 459,\n",
              " 'curved': 4414,\n",
              " 'cree': 4288,\n",
              " 'led': 9634,\n",
              " 'light': 9757,\n",
              " 'bar': 2268,\n",
              " '4x4': 659,\n",
              " 'offroad': 11739,\n",
              " 'fog': 6467,\n",
              " 'lamp': 9493,\n",
              " 're': 13389,\n",
              " 'o097vsotxk': 11619,\n",
              " 'i23xy7iejj': 8089,\n",
              " 'severe': 14522,\n",
              " 'weather': 17486,\n",
              " 'bulletin': 3073,\n",
              " 'typhoon': 16684,\n",
              " 'ûï': 18429,\n",
              " 'hannaph': 7473,\n",
              " 'soudelor': 15063,\n",
              " 'tropical': 16532,\n",
              " 'warning': 17391,\n",
              " 'issued': 8615,\n",
              " '00': 0,\n",
              " 'pm': 12594,\n",
              " '06': 23,\n",
              " 'thhjjw51pe': 16092,\n",
              " 'fits': 6357,\n",
              " '01': 6,\n",
              " 'bmw': 2754,\n",
              " '325ci': 478,\n",
              " '5l': 726,\n",
              " 'l6': 9449,\n",
              " 'gbvdnczjou': 6893,\n",
              " 'c211hise0r': 3169,\n",
              " 'explosion': 6003,\n",
              " 'london': 9928,\n",
              " 'united': 16851,\n",
              " 'kingdom': 9267,\n",
              " '10': 87,\n",
              " 'chemical': 3605,\n",
              " 'park': 12186,\n",
              " 'western': 17541,\n",
              " 'germany': 6945,\n",
              " 'xbznu0qkvs': 17948,\n",
              " 'bomb': 2789,\n",
              " 'soul': 15065,\n",
              " 'food': 6488,\n",
              " 'sound': 15069,\n",
              " 'so': 14969,\n",
              " 'terrorism': 15960,\n",
              " 'truth': 16561,\n",
              " 'bejftygjil': 2461,\n",
              " 'news': 11295,\n",
              " 'bbc': 2353,\n",
              " 'islam': 8595,\n",
              " 'isis': 8593,\n",
              " 'quran': 13207,\n",
              " 'lies': 9745,\n",
              " 'jlczidz7vu': 8866,\n",
              " 'sinking': 14765,\n",
              " 'your': 18187,\n",
              " 'lost': 9983,\n",
              " 'alone': 1569,\n",
              " 'stone': 15374,\n",
              " 'carry': 3373,\n",
              " 'onå': 11862,\n",
              " 'hostage': 7911,\n",
              " 'chicago': 3624,\n",
              " 'mylittlepwnies3': 11080,\n",
              " 'early__may': 5428,\n",
              " 'anathemazhiv': 1645,\n",
              " 'tonysandos': 16320,\n",
              " 'which': 17587,\n",
              " 'has': 7522,\n",
              " 'do': 5103,\n",
              " 'lebanon': 9631,\n",
              " '80s': 943,\n",
              " 'iran': 8561,\n",
              " 'crisis': 4311,\n",
              " 'libya': 9740,\n",
              " 'pan': 12140,\n",
              " 'am': 1595,\n",
              " 'pa': 12092,\n",
              " 'pulls': 13002,\n",
              " 'gun': 7315,\n",
              " 'man': 10245,\n",
              " 'apparent': 1779,\n",
              " 'provocation': 12948,\n",
              " 'lhw4vtbhzg': 9729,\n",
              " 'via': 17141,\n",
              " 'dailykos': 4488,\n",
              " 'derailment': 4784,\n",
              " 'minneapolis': 10684,\n",
              " 'mn': 10776,\n",
              " 'train': 16424,\n",
              " 'patna': 12236,\n",
              " 'casualty': 3407,\n",
              " 'far': 6130,\n",
              " 'indian': 8352,\n",
              " 'express': 6011,\n",
              " 'yh5vetm0yz': 18131,\n",
              " '17wgug8z0m': 197,\n",
              " 'panic': 12151,\n",
              " 'dream': 5228,\n",
              " 'magic': 10189,\n",
              " 'linden': 9791,\n",
              " 'method': 10548,\n",
              " 'lite': 9821,\n",
              " 'version': 17119,\n",
              " 'anxiety': 1733,\n",
              " 'cure': 4403,\n",
              " 'program': 12880,\n",
              " '073izwx0lb': 28,\n",
              " 'lind': 9789,\n",
              " 'okmlagvkjv': 11790,\n",
              " 'rescued': 13638,\n",
              " 'jammu': 8730,\n",
              " 'kashmir': 9098,\n",
              " 'delhi': 4711,\n",
              " '18': 198,\n",
              " 'bovines': 2849,\n",
              " 'smugglersåênabbed': 14936,\n",
              " 'e7fn5g5ruu': 5411,\n",
              " 'fredericksburg': 6615,\n",
              " 'virginia': 17202,\n",
              " 'wwp': 17901,\n",
              " 'serving': 14504,\n",
              " 'more': 10873,\n",
              " '75k': 881,\n",
              " 'veterans': 17126,\n",
              " '52k': 684,\n",
              " 'oif': 11768,\n",
              " 'oef': 11702,\n",
              " 'vets': 17127,\n",
              " 'physical': 12441,\n",
              " 'many': 10271,\n",
              " 'invisible': 8530,\n",
              " 'ones': 11840,\n",
              " 'shhlv4dplz': 14610,\n",
              " 'client': 3801,\n",
              " 'dust': 5351,\n",
              " '20storm': 339,\n",
              " 'learned': 9622,\n",
              " 'about': 1209,\n",
              " 'economics': 5479,\n",
              " 'south': 15079,\n",
              " 'dakota': 4492,\n",
              " 'storm': 15389,\n",
              " 'did': 4909,\n",
              " 'years': 18101,\n",
              " 'college': 3922,\n",
              " 'hubert': 7977,\n",
              " 'humphrey': 8012,\n",
              " 'wrecked': 17847,\n",
              " 'cramer': 4250,\n",
              " 'iger': 8195,\n",
              " 'words': 17783,\n",
              " 'disney': 5008,\n",
              " 'stock': 15365,\n",
              " 'sf5jdnvdw9': 14534,\n",
              " 'til_now': 16200,\n",
              " 'cnbc': 3849,\n",
              " 'tring': 16510,\n",
              " 'marc_holl': 10281,\n",
              " 'nennicook': 11260,\n",
              " 'aitchkaycee': 1465,\n",
              " 'vixstuart': 17228,\n",
              " 'benjbeckwith': 2497,\n",
              " 'pretty': 12817,\n",
              " 'disaster': 4979,\n",
              " 'gbbo': 6886,\n",
              " 'obliteration': 11663,\n",
              " 'canada': 3280,\n",
              " 'need': 11240,\n",
              " 'arcade': 1832,\n",
              " 'shooter': 14646,\n",
              " 'fix': 6362,\n",
              " 'cte': 4367,\n",
              " 'empty': 5673,\n",
              " 'only': 11850,\n",
              " 'running': 14056,\n",
              " 'even': 5896,\n",
              " 'buy': 3138,\n",
              " 'cod': 3885,\n",
              " 'title': 16234,\n",
              " 'weren': 17533,\n",
              " 'overpriced': 12032,\n",
              " 'steam': 15324,\n",
              " 'bioterrorism': 2609,\n",
              " 'firepower': 6342,\n",
              " 'lab': 9462,\n",
              " 'electronic': 5586,\n",
              " 'resource': 13659,\n",
              " 'automation': 2075,\n",
              " 'against': 1409,\n",
              " 'infectious': 8386,\n",
              " 'diseases': 4998,\n",
              " 'kvpbybglsr': 9415,\n",
              " 'graysondolan': 7201,\n",
              " 'me': 10434,\n",
              " 'explode': 5995,\n",
              " 'washington': 17412,\n",
              " 'kendall': 9160,\n",
              " 'jenner': 8806,\n",
              " 'nick': 11339,\n",
              " 'jonas': 8915,\n",
              " 'dating': 4573,\n",
              " 'quite': 13196,\n",
              " 'literally': 9822,\n",
              " 'pfvzvpxqgr': 12399,\n",
              " 'always': 1594,\n",
              " 'tell': 15913,\n",
              " 'mom': 10823,\n",
              " 'bring': 2963,\n",
              " 'hold': 7813,\n",
              " 'cat': 3408,\n",
              " 'heat': 7616,\n",
              " '20wave': 342,\n",
              " 'fort': 6540,\n",
              " 'worth': 17816,\n",
              " 'rt': 14016,\n",
              " 'startelegram': 15290,\n",
              " 'homeless': 7842,\n",
              " 'vulnerable': 17297,\n",
              " 'north': 11463,\n",
              " 'wave': 17440,\n",
              " 'k9airfq3ql': 9049,\n",
              " 'jdbtlymehy': 8790,\n",
              " 'nuclear': 11547,\n",
              " '20reactor': 333,\n",
              " 'solar': 14997,\n",
              " 'power': 12718,\n",
              " 'japanese': 8748,\n",
              " 'fukushima': 6717,\n",
              " 'reactor': 13399,\n",
              " 'energy': 5710,\n",
              " 'japan': 8747,\n",
              " 'temperature': 15924,\n",
              " 'fuel': 6710,\n",
              " 'pool': 12655,\n",
              " 'ys3nmwwyvc': 18215,\n",
              " 'alpotnb7q3': 1574,\n",
              " 'arvada': 1908,\n",
              " 'least': 9626,\n",
              " 'taken': 15769,\n",
              " 'local': 9900,\n",
              " 'wlmsq3mtho': 17725,\n",
              " 'trauma': 16451,\n",
              " 'nashville': 11161,\n",
              " 'tn': 16264,\n",
              " 'esteemed': 5852,\n",
              " 'journalist': 8936,\n",
              " 'recalls': 13445,\n",
              " 'tragic': 16420,\n",
              " 'effects': 5529,\n",
              " 'unaddressed': 16797,\n",
              " 'childhood': 3636,\n",
              " 'keithboykin': 9149,\n",
              " 'randallpinkston': 13317,\n",
              " 'pozarmy': 12723,\n",
              " 'gxq1auzb18': 7362,\n",
              " 'panicking': 12152,\n",
              " 'feel': 6213,\n",
              " 'results': 13689,\n",
              " 'back': 2179,\n",
              " 'alarmingly': 1498,\n",
              " 'calm': 3236,\n",
              " 'lightning': 9763,\n",
              " 'thunder': 16168,\n",
              " 'possible': 12689,\n",
              " 'pinpoint': 12479,\n",
              " 'foothill': 6502,\n",
              " 'forecast': 6511,\n",
              " 'ctijdpxabk': 4369,\n",
              " 'displaced': 5019,\n",
              " '40': 562,\n",
              " 'ocean': 11684,\n",
              " 'township': 16385,\n",
              " 'apartment': 1756,\n",
              " 'newyork': 11308,\n",
              " 'uelz59wvom': 16749,\n",
              " 'massacre': 10345,\n",
              " 'stay': 15315,\n",
              " 'tuned': 16602,\n",
              " 'freddiedeboer': 6612,\n",
              " 'thucydiplease': 16166,\n",
              " 'rise': 13810,\n",
              " 'coates': 3874,\n",
              " 'charleston': 3568,\n",
              " 'walter': 17366,\n",
              " 'black': 2652,\n",
              " 'twitter': 16656,\n",
              " 'broadly': 2982,\n",
              " 'well': 17519,\n",
              " 'deaths': 4636,\n",
              " 'gallifrey': 6826,\n",
              " 'mathew_is_angry': 10366,\n",
              " 'z3ke_sk1': 18256,\n",
              " 'saladinahmed': 14154,\n",
              " 'died': 4915,\n",
              " 'horrible': 7898,\n",
              " 'trapped': 16448,\n",
              " 'ships': 14624,\n",
              " 'risk': 13813,\n",
              " '20buildings': 318,\n",
              " 'whiterun': 17601,\n",
              " 'skyrim': 14840,\n",
              " 'destruction': 4829,\n",
              " 'fine': 6315,\n",
              " 'just': 9003,\n",
              " 'windstorm': 17667,\n",
              " 'palm': 12132,\n",
              " 'county': 4202,\n",
              " 'fl': 6379,\n",
              " 'reality': 13418,\n",
              " 'training': 16426,\n",
              " 'falls': 6103,\n",
              " 'off': 11716,\n",
              " 'elevated': 5597,\n",
              " 'tracks': 16405,\n",
              " 'jiomnrcygt': 8854,\n",
              " 'paramedic': 12173,\n",
              " 'ems': 5675,\n",
              " 'rescuers': 13639,\n",
              " 'fears': 6199,\n",
              " 'missing': 10721,\n",
              " 'migrants': 10625,\n",
              " 'med': 10448,\n",
              " 'search': 14378,\n",
              " 'survivors': 15627,\n",
              " 'boat': 2767,\n",
              " 'carrying': 3375,\n",
              " '6ds67xai5e': 810,\n",
              " 'derailed': 4782,\n",
              " 'toronto': 16346,\n",
              " 'derailed_benchmark': 4783,\n",
              " 'cool': 4127,\n",
              " 'paths': 12232,\n",
              " 'wonder': 17762,\n",
              " 'can': 3277,\n",
              " 'find': 6312,\n",
              " 'leaks': 9619,\n",
              " 'jobs': 8885,\n",
              " 'given': 7016,\n",
              " 'resque': 13673,\n",
              " 'too': 16321,\n",
              " 'ladies': 9473,\n",
              " 'tote': 16362,\n",
              " 'handbag': 7458,\n",
              " 'women': 17757,\n",
              " 'faux': 6162,\n",
              " 'leather': 9627,\n",
              " 'fashion': 6144,\n",
              " 'purse': 13040,\n",
              " 'y87gi3brlv': 18057,\n",
              " '1zbhvdcxzs': 280,\n",
              " 'raishimi33': 13302,\n",
              " 'sounds': 15073,\n",
              " 'plan': 12537,\n",
              " 'little': 9827,\n",
              " 'applaud': 1785,\n",
              " 'catastrophic': 3414,\n",
              " 'buxton': 3137,\n",
              " 'venice': 17105,\n",
              " 'nottingham': 11502,\n",
              " 'invading': 8518,\n",
              " 'iraq': 8566,\n",
              " 'mistake': 10730,\n",
              " 'diplomacy': 4951,\n",
              " 'needs': 11244,\n",
              " 'replace': 13611,\n",
              " 'constant': 4084,\n",
              " 'threat': 16137,\n",
              " 'war': 17380,\n",
              " 'israel': 8612,\n",
              " 'yqjpn3quux': 18205,\n",
              " 'related': 13543,\n",
              " 'threatens': 16140,\n",
              " 'europe': 5881,\n",
              " 'wk6b5z803o': 17720,\n",
              " 'livingston': 9852,\n",
              " 'mt': 10982,\n",
              " 'marynmck': 10335,\n",
              " 'beyond': 2537,\n",
              " 'adorable': 1334,\n",
              " 'hope': 7882,\n",
              " 'won': 17761,\n",
              " 'been': 2431,\n",
              " 'noticed': 11491,\n",
              " 'devastation': 4849,\n",
              " 'mount': 10923,\n",
              " 'vernon': 17115,\n",
              " 'coming': 3962,\n",
              " 'target': 15803,\n",
              " 'starbucks': 15279,\n",
              " 'closed': 3819,\n",
              " 'momneedscoffee': 10827,\n",
              " 'asap': 1915,\n",
              " 'iwontmakeit': 8651,\n",
              " 'bombed': 2791,\n",
              " 'screwston': 14353,\n",
              " 'tx': 16664,\n",
              " 'redskins': 13491,\n",
              " 'wr': 17838,\n",
              " 'roberts': 13868,\n",
              " 'belly': 2478,\n",
              " 'teamstream': 15877,\n",
              " 'gbcvvevdty': 6887,\n",
              " 'hellfire': 7650,\n",
              " 'allah': 1540,\n",
              " 'describes': 4795,\n",
              " 'piling': 12470,\n",
              " 'wealth': 17478,\n",
              " 'thinking': 16100,\n",
              " 'forever': 6519,\n",
              " 'surah': 15599,\n",
              " 'humaza': 8003,\n",
              " 'reflect': 13505,\n",
              " 'worldwide': 17802,\n",
              " 'loved': 9998,\n",
              " 'way': 17445,\n",
              " 'written': 17861,\n",
              " 'include': 8327,\n",
              " 'vantage': 17057,\n",
              " 'points': 12625,\n",
              " 'detkenlang': 4838,\n",
              " 'kindle': 9259,\n",
              " 'kcrnmjkj73': 9131,\n",
              " 'heartdisease': 7609,\n",
              " 'service': 14501,\n",
              " 'spending': 15140,\n",
              " 'half': 7436,\n",
              " 'budget': 3052,\n",
              " 'kzfigkeeva': 9434,\n",
              " 'tragedy': 16419,\n",
              " 'sandrabland': 14200,\n",
              " 'forget': 6522,\n",
              " 'gajtugaui7': 6817,\n",
              " 'mayhem': 10395,\n",
              " 'detroit': 4842,\n",
              " 'liked': 9770,\n",
              " 'youtube': 18195,\n",
              " 'itsjustinstuart': 8632,\n",
              " 'mnkaji2q1n': 10777,\n",
              " 'range': 13325,\n",
              " 'hungerarticles': 8019,\n",
              " 'nepal': 11264,\n",
              " 'rebuilding': 13440,\n",
              " 'livelihoods': 9839,\n",
              " 'quake': 13161,\n",
              " 'lrouwjmbix': 10026,\n",
              " 'arsonist': 1892,\n",
              " 'suspected': 15633,\n",
              " 'serial': 14492,\n",
              " 'arrested': 1882,\n",
              " 'calif': 3228,\n",
              " 'pzotpdgaki': 13075,\n",
              " 'hurricane': 8036,\n",
              " 'vineyard': 17187,\n",
              " 'chubbysquirrel_': 3702,\n",
              " 'hurricane_surge': 8039,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# напишем функциию, которая поочереди будет возвращать нам ответы на вышеперечисленные вопросы\n",
        "import string \n",
        "punctuation1 = string.punctuation\n",
        "punctuation = punctuation1.replace('#', '').replace('@', '')\n",
        "# избавимся от символов # и @\n",
        "\n",
        "def info(x : dict):\n",
        "  keys = list(x.keys())\n",
        "  n = 0\n",
        "  k = 0\n",
        "  m = 0\n",
        "  for i in keys:\n",
        "    if any(map(str.isdigit, i)) == True:\n",
        "      n += 1\n",
        "    if len(set(i) & set(punctuation)) != 0:\n",
        "      k += 1\n",
        "    if i[0] == '@' or i[0] == '#':\n",
        "      m += 1\n",
        "  print(f'Words with numbers - {n} \\nWords with punctuations - {k} \\nWords with @ or # - {m}')\n",
        "  return n , k, m\n",
        "info(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD7gYOsimwnY",
        "outputId": "75479b6d-279a-4ebd-c704-e14690721220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words with numbers - 3812 \n",
            "Words with punctuations - 315 \n",
            "Words with @ or # - 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3812, 315, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "help(TweetTokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqhxuKUeA4eU",
        "outputId": "dfa2b36a-8a11-4e28-8b49-d763cfa2fec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class TweetTokenizer in module nltk.tokenize.casual:\n",
            "\n",
            "class TweetTokenizer(builtins.object)\n",
            " |  TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False)\n",
            " |  \n",
            " |  Tokenizer for tweets.\n",
            " |  \n",
            " |      >>> from nltk.tokenize import TweetTokenizer\n",
            " |      >>> tknzr = TweetTokenizer()\n",
            " |      >>> s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
            " |      >>> tknzr.tokenize(s0)\n",
            " |      ['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n",
            " |  \n",
            " |  Examples using `strip_handles` and `reduce_len parameters`:\n",
            " |  \n",
            " |      >>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
            " |      >>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
            " |      >>> tknzr.tokenize(s1)\n",
            " |      [':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, preserve_case=True, reduce_len=False, strip_handles=False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  tokenize(self, text)\n",
            " |      :param text: str\n",
            " |      :rtype: list(str)\n",
            " |      :return: a tokenized list of strings; concatenating this list returns        the original string if `preserve_case=False`\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQAOHGdzgvHu"
      },
      "source": [
        "## Задание 6 (0.5 балла)\n",
        "\n",
        "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkz-fjJhgvHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f57f5ec-a77a-4703-9314-c39113a562bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words with numbers - 3940 \n",
            "Words with punctuations - 4474 \n",
            "Words with @ or # - 3149\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3940, 4474, 3149)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "# Чтобы узнать, какие параметры есть у этого токенайзера - используйте help(TweetTokenizer)\n",
        "# Для того, чтобы передать токенайзер в CountVectorizer используйте параметр tokenizer\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "tweet_vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize)\n",
        "tweet_vectorizer.fit(X_train)\n",
        "tweet_vectorizer.vocabulary_\n",
        "info(tweet_vectorizer.vocabulary_)\n",
        "\n",
        "# Tweet Tokenizer лучше \"ловит\" слова с упоминаниями и хэштегами, а также слова со знаками препинания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsNqiHgbgvHv"
      },
      "source": [
        "## Задание 7 (2 балла)\n",
        "\n",
        "В scikit-learn мы можем оценивать процесс подсчета матрицы через CountVectorizer. У CountVectorizer, как и у других наследников \\_VectorizerMixin, есть аргумент tokenizer и preprocessor. preprocessor применится в самом начале к каждой строке вашего датасета, tokenizer же должен принять строку и вернуть токены.\n",
        "Давайте напишем кастомный токенайзер, которые сделает все, что нам нужно: \n",
        "\n",
        "0. Приведет все буквы к нижнему регистру\n",
        "1. Разобьет текст на токены с помощью TweetTokenizer из пакета nltk\n",
        "2. Удалит все токены содержащие не латинские буквы, кроме смайликов (будем считать ими токены содержащие только пунктуацию и, как минимум, одну скобочку) и хэштегов, которые после начальной # содержат только латинские буквы.\n",
        "3. Удалит все токены, которые перечислены в nltk.corpus.stopwords.words('english')\n",
        "4. Проведет стемминг с помощью SnowballStemmer\n",
        "\n",
        "Продемонстрируйте работу вашей функции на первых десяти текстах в обучающей выборке."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "skobki = '(){}[]'\n",
        "letters = [chr(i) for i in range(97, 123)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-txXjphQ3cU",
        "outputId": "d1028cf3-045a-4f6f-d8e0-98ae621490ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def islatin(x : str):\n",
        "  letters = [chr(i) for i in range(97, 123)]\n",
        "  func = np.vectorize(lambda x: x in letters)\n",
        "  return all(func(list(x)))\n",
        "\n",
        "\n",
        "def issmilik(x : str):\n",
        "  skobki = '(){}[]'\n",
        "  func1 = np.vectorize(lambda x: x in skobki)\n",
        "  func2 = np.vectorize(lambda x: x in punctuation)\n",
        "  return any(func1(list(x))) and all(func2(list(x)))\n",
        "\n",
        "\n",
        "def ishashtag(x : str):\n",
        "  letters = [chr(i) for i in range(97, 123)]\n",
        "  func = np.vectorize(lambda x: x in letters)\n",
        "  if len(x) > 1:\n",
        "    ans = all(func(list(x[1:])))\n",
        "  else:\n",
        "    ans = False\n",
        "  return list(x)[0] == '#' and ans"
      ],
      "metadata": {
        "id": "9qOIHhpLNdKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_tokenizer(x : str):\n",
        "  lower_x = x.lower() #0\n",
        "\n",
        "  tk = TweetTokenizer() \n",
        "\n",
        "  a = tk.tokenize(lower_x) #1\n",
        "\n",
        "  for i in a:\n",
        "    if islatin(i) != True and ishashtag(i) != True and issmilik(i) != True:\n",
        "      a.remove(i)                                                             #2\n",
        "\n",
        "  for i in a:\n",
        "    if i in stopwords:\n",
        "      a.remove(i)       #3   \n",
        "\n",
        "  snowball = SnowballStemmer(language='english')\n",
        "  a = list(map(lambda x: snowball.stem(x), a))   #4\n",
        "\n",
        "  return a"
      ],
      "metadata": {
        "id": "gjbMMxX3vqtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in X_train[:10]:\n",
        "  print(my_tokenizer(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUqWphM5vxJQ",
        "outputId": "ffc841a3-d3fb-4d72-e2c2-8e852adf0153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bridg', '20collaps', 'ash', 'australia', 'ûªs', 'collaps', 'trent', 'bridg', 'among', 'worst', 'histori', ':', 'england', 'bundl', 'australia', '...']\n",
            "['hail', 'carol', 'stream', 'illinoi', 'great', 'michigan', 'techniqu', 'camp', 'thank', '@hail_youtsey', '@termn8r13', '#goblu', '#wrestleon']\n",
            "['polic', 'houston', 'cnn', 'tennesse', 'movi', 'theater', 'shoot', 'suspect', 'kill', 'polic']\n",
            "['riot', 'still', 'riot', 'a', 'coupl', 'hour', 'left', 'i', 'to', 'up', 'class']\n",
            "['wound', 'lake', 'highland', 'crack', 'the', 'path', 'i', 'wipe', 'this', 'morn', 'beach', 'run', 'surfac', 'wound', 'left', 'elbow', 'right', 'knee', 'http://t.co/yaqrsximph']\n",
            "['airplan', '20accid', 'somewher', 'there', 'expert', 'franc', 'begin', 'examin', 'airplan', 'debri', 'found', 'reunion', 'island', 'french', 'air', 'accid', 'expert', 'http://t.co/tagzbcxfj0', '#mlb']\n",
            "['bloodi', 'isol', 'citi', 'world', 'perth', 'came', 'kill', 'indian', 'fun', ':', 'video', 'smirk', 'remorseless', 'pakistani', 'killer', 'show', 'boast', 'http://t.co/fpjlwoxklg']\n",
            "['burn', 'except', 'idk', 'realli', 'burn', '?', '?']\n",
            "['destroy', 'him', 'she', '(', 'ask', ')', 'destroy', 'hous']\n",
            "['wound', 'maracay', 'nirgua', 'venezuela', 'polic', 'offic', 'wound', 'suspect', 'dead', 'exchang', 'shot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1qz4aVYgvHv"
      },
      "source": [
        "## Задание 8 (1 балл)\n",
        "\n",
        "1. Примените CountVectorizer с реализованным выше токенизатором к обучающим и тестовым выборкам.\n",
        "2. Обучите LogisticRegression на полученных признаках.\n",
        "3. Посчитайте метрику f1-score на тестовых данных."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "my_vectorizer_train = CountVectorizer(tokenizer=my_tokenizer)\n",
        "my_vectorizer_train.fit(X_train)\n",
        "tr = my_vectorizer_train.transform(X_train)\n",
        "ts = my_vectorizer_train.transform(X_test)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(tr, y_train)\n",
        "\n",
        "print(f'\\nf1 score: {f1_score(logreg.predict(ts), y_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aDcy-Ke2zhL",
        "outputId": "e427846d-79fd-48a4-fcb0-c5540c9398d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score: 0.7456709956709956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9isP4c3lgvHw"
      },
      "source": [
        "## Задание 9 (1 балл)\n",
        "\n",
        "1. Повторите 7 задание, но с tf-idf векторизатором. Как изменилось качество?\n",
        "2. Мы можем еще сильнее уменьшить размер нашей матрицы, если отбросим значения df близкие к единице. Скорее всего такие слова не несут много информации о категории, так как встречаются достаточно часто. Ограничьте максимальный df в параметрах TfIdfVectorizer, поставьте верхнюю границу равную 0.9. Как изменился размер матрицы, как изменилось качество?\n",
        "3. Также мы можем уменьшить размер матрицы, удаляя слова со слишком маленьким df. Удалось ли добиться улучшения качества? "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vct = TfidfVectorizer(tokenizer=my_tokenizer) \n",
        "vct.fit(X_train)\n",
        "tr2 = vct.transform(X_train)\n",
        "ts2 = vct.transform(X_test)\n",
        "\n",
        "logreg2 = LogisticRegression()\n",
        "logreg2.fit(tr2, y_train)\n",
        "\n",
        "print(f'\\nf1-score: {f1_score(logreg2.predict(ts2), y_test)}')\n",
        "print(f'Размер матрицы без max_df: {tr2.shape}')\n",
        "#качество немного ухудшилось, но не значительно"
      ],
      "metadata": {
        "id": "XX-slagvvaLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0472ec03-3249-41dd-a8ca-b84ac1fdd27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score: 0.7440310938367575\n",
            "Размер матрицы без max_df: (5329, 12833)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#теперь добавим максимальное значение df равное 0.9 и проверим качество\n",
        "vct = TfidfVectorizer(tokenizer=my_tokenizer, max_df=0.9) \n",
        "vct.fit(X_train)\n",
        "tr3 = vct.transform(X_train)\n",
        "ts3 = vct.transform(X_test)\n",
        "\n",
        "logreg3 = LogisticRegression()\n",
        "logreg3.fit(tr3, y_train)\n",
        "\n",
        "print(f'\\nf1-score: {f1_score(logreg3.predict(ts3), y_test)}')\n",
        "print(f'Размер матрицы max_df = 0.9: {tr3.shape}')\n",
        "# качество не изменилось, и размер матрицы так же не поменялся, возможно, потому что нет элементов \n",
        "#  встречающихся так часто"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkHgqND9495s",
        "outputId": "d4a8d57e-d884-45db-9d62-379edf5c03b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score: 0.7440310938367575\n",
            "Размер матрицы max_df = 0.9: (5329, 12833)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# теперь будем удалять слова с маленьким df\n",
        "vct = TfidfVectorizer(tokenizer=my_tokenizer, min_df = 0.0002) \n",
        "vct.fit(X_train)\n",
        "tr4 = vct.transform(X_train)\n",
        "ts4 = vct.transform(X_test)\n",
        "\n",
        "logreg4 = LogisticRegression()\n",
        "logreg4.fit(tr4, y_train)\n",
        "\n",
        "print(f'\\nf1-score: {f1_score(logreg4.predict(ts4), y_test)}')\n",
        "print(f'Размер матрицы : {tr4.shape}')\n",
        "# при установлении df_min = 0.0002 матрица значительно уменьшилась в размерах и качество стало чуть лучше"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJNqInBe6JY9",
        "outputId": "d33589ad-b670-475c-d515-deb4f352d561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score: 0.7473625763464742\n",
            "Размер матрицы : (5329, 4821)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fyZ72clgvHw"
      },
      "source": [
        "## Задание 10 (1 балл)\n",
        "\n",
        "Еще один популяпный трюк, который позволит уменьшить количество признаков называется hashing trick. Его суть в том, то мы случайно группируем признаки ииии  ..... складываем их! А потом удаляем исходные признаки. В итоге все наши признаки это просто суммы исходных. Звучит странно, но это отлично работает. Давайте проверим этот трюк в нашем сеттинге.\n",
        "Также при таком подходе вам не нужно хранить словарь token->index, что тоже иногда полезно.\n",
        "\n",
        "1. Повторите задание 7 с HashingVectorizer, укажите количество фичей равное 5000.\n",
        "2. Какой из подходов показал самый высокий результат?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "my_vectorizer_train = HashingVectorizer(tokenizer=my_tokenizer, n_features=5000)\n",
        "my_vectorizer_train.fit(X_train)\n",
        "tr = my_vectorizer_train.transform(X_train)\n",
        "ts = my_vectorizer_train.transform(X_test)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(tr, y_train)\n",
        "\n",
        "print(f'\\nf1 score: {f1_score(logreg.predict(ts), y_test)}')\n",
        "\n",
        "# при n_features = 5000 качество ухудшилось\n",
        "\n",
        "# самый хороший результат показала модель с TfidfVectorizer и df_min = 0.0002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmuCyjiyN7Vi",
        "outputId": "90aed02b-afba-4fc2-9f7e-0565f74a309d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score: 0.7191513121161363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSYraNU6gvHw"
      },
      "source": [
        "## Задание 11 (1 балл)\n",
        "\n",
        "В этом задании нужно добиться f1 меры хотя в 0.75 на тестовых данных."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# несмотря на то что у нас уже получилась модель с f1-score = 0,75 (если округлить до стоых), попробуем поподбирать гиперпараметры модели с TfidfVectorizer и TfidfVectorizer\n",
        "# начнем с первой \n",
        "df_min = [i for i in np.arange(0.0003, 0.0015, 0.0002)]\n",
        "for i in df_min:\n",
        "  vct = TfidfVectorizer(tokenizer=my_tokenizer, min_df = i, max_df = 0.1 )\n",
        "  vct.fit(X_train)\n",
        "  tr4 = vct.transform(X_train)\n",
        "  ts4 = vct.transform(X_test)\n",
        "\n",
        "  logreg4 = LogisticRegression()\n",
        "  logreg4.fit(tr4, y_train)\n",
        "\n",
        "  print(f'\\nf1-score (df_min = {i}): {f1_score(logreg4.predict(ts4), y_test)}')\n",
        "  print(f'Размер матрицы: {tr4.shape}')\n",
        "\n",
        "  # перебрав несколько значений min_df при max_df = 0.1, которые влияют на размер матрицы, удалось немного улучшить качество и получить f1-score практически равным 0.75 (0.74986)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq7l-w-PRnmW",
        "outputId": "19972739-8abe-4739-b709-e3ac2ef41e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score (df_min = 0.0003): 0.7498613422074321\n",
            "Размер матрицы max_df = 0.9: (5329, 4819)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score (df_min = 0.0005): 0.7444444444444446\n",
            "Размер матрицы max_df = 0.9: (5329, 3371)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score (df_min = 0.0007000000000000001): 0.7461368653421633\n",
            "Размер матрицы max_df = 0.9: (5329, 2645)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score (df_min = 0.0009): 0.7436743674367436\n",
            "Размер матрицы max_df = 0.9: (5329, 2217)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score (df_min = 0.0011): 0.7425742574257426\n",
            "Размер матрицы max_df = 0.9: (5329, 1920)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1-score (df_min = 0.0013000000000000002): 0.7423245614035087\n",
            "Размер матрицы max_df = 0.9: (5329, 1684)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#теперь попробуем поподбирать гипер параметры в модели с TfidfVectorizer\n",
        "n_features = np.arange(100000, 1100000, 100000)\n",
        "\n",
        "for i in n_features:\n",
        "  my_vectorizer_train = HashingVectorizer(tokenizer=my_tokenizer, n_features=i)\n",
        "  my_vectorizer_train.fit(X_train)\n",
        "  tr = my_vectorizer_train.transform(X_train)\n",
        "  ts = my_vectorizer_train.transform(X_test)\n",
        "\n",
        "  logreg = LogisticRegression()\n",
        "  logreg.fit(tr, y_train)\n",
        "\n",
        "  print(f'\\nf1 score (n_features = {i}): {f1_score(logreg.predict(ts), y_test)}')\n",
        "\n",
        "# перебрав значения гиперпараметра разных порядков, не удается достичь такого же хорошего качества как на модели TfidfVectorizer с df_min = 0.0003 и  df_max = 0.1 \n",
        "# а значит, что модель дающая нам f1-score = 0,75 (если округлить до стоых) - это именно модель TfidfVectorizer с df_min = 0.0003 и  df_max = 0.1   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ath9mSsUWNJe",
        "outputId": "bba856c0-e4a7-42f0-ee33-ffd2555ad28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 100000): 0.7374301675977653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 200000): 0.7388392857142856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 300000): 0.7404921700223713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 400000): 0.7407821229050279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 500000): 0.7418994413407821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 600000): 0.7414852037967614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 700000): 0.7400782560089434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 800000): 0.7411961989938514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 900000): 0.7385474860335195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f1 score (n_features = 1000000): 0.7418994413407821\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "ДЗ 6",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}